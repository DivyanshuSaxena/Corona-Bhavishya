{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import importlib as imp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.losses import Huber\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User module imports\n",
    "from utils import district_daily_data as dd\n",
    "from utils import state_daily_data as sd\n",
    "from utils import get_lockdown as gl\n",
    "\n",
    "dd = imp.reload(dd)\n",
    "sd = imp.reload(sd)\n",
    "gl = imp.reload(gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "lstm_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory variables\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read state data\n",
    "# df_state = pd.read_csv(data_dir + 'state-date-total-data.csv')\n",
    "# df_state.rename(columns=lambda s: s[3:], inplace=True)\n",
    "# arr_state = df_state.to_numpy() # still reversed\n",
    "# arr_state = np.flipud(arr_state) # now taken data from day-1 to day-52; but still daily cases\n",
    "# arr_state = np.cumsum(arr_state, axis=0) # now cumulative cases till day 52\n",
    "# np.savetxt(data_dir + 'state-date-total-data-cumulative.csv', arr_state.astype(int), fmt='%i', delimiter=\",\")\n",
    "\n",
    "# Read population density data\n",
    "df_population_density = pd.read_csv(data_dir + 'district_wise_population_density.csv')\n",
    "np_population_density = df_population_density.to_numpy() \n",
    "data_found_count = 0 # no of districts for which we have population density data\n",
    "\n",
    "def get_district_population_density(d):\n",
    "    global data_found_count\n",
    "    dist_pop_density = -2\n",
    "    for i_cn in range(len(np_population_density)):\n",
    "        if(np_population_density[i_cn][1].lower().count(d.lower().strip()) > 0):\n",
    "            dist_pop_density = max(float(np_population_density[i_cn][7]), dist_pop_density)\n",
    "    if(dist_pop_density <= 0): # print(d) # district not matched || area not found || population data missing\n",
    "        dist_pop_density = 368 # population density of INDIA\n",
    "    else:\n",
    "        data_found_count = data_found_count + 1\n",
    "    return dist_pop_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read district data\n",
    "districts = dd.get_all_districts()\n",
    "dist_series = []  # [(start_date, series), (start_date, series), ...]\n",
    "max_number = 0\n",
    "\n",
    "# Note: start_date might itself be a feature\n",
    "for d in districts:\n",
    "    if d == \"Mumbai\" or d == \"Thane\" or d == \"Delhi\" or d == \"Lucknow\":\n",
    "#     if d == \"Mumbai\":\n",
    "        d_start_date = dd.get_infection_start(d)\n",
    "        district_pop_density = get_district_population_density(d)\n",
    "        district_time_series = dd.get_district_time_series(d, d_start_date)\n",
    "        lockdown_time_series = gl.get_lockdown_series(d, d_start_date, len(district_time_series))\n",
    "        dist_series.append((d, district_time_series, lockdown_time_series, district_pop_density))\n",
    "        \n",
    "        # Update district max to scale all numbers\n",
    "        district_max = max(district_time_series)\n",
    "        if district_max > max_number:\n",
    "            max_number = district_max\n",
    "\n",
    "print(dist_series)\n",
    "print(\"data_found_count:\", data_found_count,  \" tot dists:\", len(districts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = 5\n",
    "\n",
    "# Transform using MinMaxScaler\n",
    "def fit_transform(series):\n",
    "    global max_number\n",
    "    series = np.array(list(map(lambda x: x/max_number*feature_range, series)))\n",
    "    return series\n",
    "\n",
    "# Revert the transform to get actual series\n",
    "def inverse_transform(series):\n",
    "    global max_number\n",
    "    series = np.array(list(map(lambda x: np.rint(x*max_number/feature_range), series)))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get separate train and test sets with data points from each of the districts\n",
    "def divide_series(dist_series, train_percent, look_ahead=1):\n",
    "    # Construct train and test data and fit Support Vector Regression\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    # Construct a list of district wise input features and outputs\n",
    "    dist_set = []\n",
    "    episode_length = 14\n",
    "    count = 0\n",
    "    for tup in dist_series:\n",
    "        # Series for infected numbers\n",
    "        series = tup[1]\n",
    "        a = np.array(series)\n",
    "        series = np.reshape(a, (a.shape[0], 1))\n",
    "        series = fit_transform(series)\n",
    "        \n",
    "        # Series for lockdown levels and population density\n",
    "        ld_series = to_categorical(tup[2])\n",
    "        dist_pop_density = tup[3]\n",
    "        pd_series = np.repeat(tup[3], len(series))\n",
    "        pd_series = np.reshape(pd_series, (pd_series.shape[0], 1))\n",
    "        \n",
    "        num_episodes = len(series) - episode_length + 1\n",
    "        if num_episodes < 2: continue\n",
    "\n",
    "        dist_x = []\n",
    "        dist_y = []\n",
    "        for _in in range(num_episodes-look_ahead):\n",
    "            # Concatenate infected numbers and lockdown levels\n",
    "            multivar_x = np.concatenate((series[_in:_in+episode_length], ld_series[\n",
    "                _in:_in+episode_length]), axis=1)\n",
    "            # Also concatenate population density along with these numbers\n",
    "            # multivar_x = np.concatenate((multivar_x, pd_series[_in:_in+episode_length]), axis=1)\n",
    "            dist_x.append(multivar_x)\n",
    "            dist_y.append(series[_in+episode_length+look_ahead-1])\n",
    "        \n",
    "        # Add to Dsitrict wise set\n",
    "        dist_set.append((tup[0], dist_x, dist_y))\n",
    "        \n",
    "        # Training and testing set\n",
    "        train_length = int(train_percent*len(dist_x))\n",
    "        x_train.extend(dist_x[:train_length])\n",
    "        y_train.extend(dist_y[:train_length])\n",
    "        x_test.extend(dist_x[train_length:-1])\n",
    "        y_test.extend(dist_y[train_length:-1])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    print (x_train.shape, x_test.shape)\n",
    "    return (x_train, y_train), (x_test, y_test), dist_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 14\n",
    "num_features = 7\n",
    "train_percent = 0.75\n",
    "\n",
    "if lstm_model:\n",
    "    train, test, dist_set = divide_series(dist_series, train_percent=train_percent, look_ahead=6)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    np.random.seed(7)\n",
    "    trainX = np.reshape(train[0], (train[0].shape[0], train[0].shape[1], num_features))\n",
    "    testX = np.reshape(test[0], (test[0].shape[0], test[0].shape[1], num_features))\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(2, input_shape=(look_back, 1), return_sequences=True))\n",
    "    model.add(LSTM(7, input_shape=(look_back, num_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=Huber(delta=50), optimizer='adam')\n",
    "    model.fit(trainX, train[1], epochs=150, batch_size=1, verbose=2)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = inverse_transform(trainPredict)\n",
    "    trainY = inverse_transform(train[1])\n",
    "    testPredict = inverse_transform(testPredict)\n",
    "    testY = inverse_transform(test[1])\n",
    "    \n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    dataset = dist_series[0][1]\n",
    "    a = np.array(dataset)\n",
    "    dataset = a.reshape(a.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs for each of the districts in the dist_series\n",
    "for tup in dist_set:\n",
    "    dist_x = np.array(tup[1])\n",
    "    dist_y = np.array(tup[2])\n",
    "    train_length = int(train_percent*len(dist_x))\n",
    "    \n",
    "    trainX = dist_x[:train_length]\n",
    "    testX = dist_x[train_length:-1]\n",
    "    trainY = dist_y[:train_length]\n",
    "    testY = dist_y[train_length:-1]\n",
    "    \n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], num_features))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], num_features))\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = inverse_transform(trainPredict)\n",
    "    trainY = inverse_transform(trainY)\n",
    "    testPredict = inverse_transform(testPredict)\n",
    "    testY = inverse_transform(testY)\n",
    "\n",
    "    print(trainY.shape, trainPredict.shape)\n",
    "    print(testY.shape, testPredict.shape)\n",
    "\n",
    "    x1 = np.arange(1, trainY.shape[0]+1)\n",
    "    x2 = np.arange(trainY.shape[0]+1, trainY.shape[0]+1+testY.shape[0])\n",
    "\n",
    "    print (tup[0] + ':')\n",
    "    # Time Series\n",
    "    plt.plot(x1, trainY)\n",
    "    plt.plot(x1, trainPredict)\n",
    "    plt.plot(x2, testY)\n",
    "    plt.plot(x2, testPredict)\n",
    "    plt.show()\n",
    "\n",
    "    # Cumulative\n",
    "    x = np.append(x1, x2)\n",
    "    y_true = np.append(trainY, testY)\n",
    "    y_pred = np.append(trainPredict, testPredict)\n",
    "    plt.plot(x, np.cumsum(y_true))\n",
    "    plt.plot(x, np.cumsum(y_pred))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State data LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sd.get_all_states()\n",
    "state_series = []  # [(start_date, series, population density), (start_date, series), ...]\n",
    "max_number = 0\n",
    "\n",
    "for s in states:\n",
    "    if s==\"Gujarat\":\n",
    "        s_start_date = sd.get_infection_start(s)\n",
    "        print(s_start_date)\n",
    "        state_time_series = sd.get_state_time_series(s, s_start_date)\n",
    "        plt.plot(np.arange(1, len(state_time_series)+1), state_time_series)\n",
    "        plt.show()\n",
    "        state_series.append((s, state_time_series))\n",
    "        # Update district max to scale all numbers\n",
    "        state_max = max(state_time_series)\n",
    "        if state_max > max_number:\n",
    "            max_number = state_max\n",
    "\n",
    "            \n",
    "print(len(state_series[0][1][:-7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get separate train and test sets with data points from each of the districts\n",
    "def divide_state_series(state_series, train_percent, look_ahead=1):\n",
    "    # Construct train and test data and fit Support Vector Regression\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    # Construct a list of district wise input features and outputs\n",
    "    state_set = []\n",
    "    episode_length = 14\n",
    "    count = 0\n",
    "    for tup in state_series:\n",
    "        # Series for infected numbers\n",
    "        series = tup[1]\n",
    "        a = np.array(series[:-7])\n",
    "        series = np.reshape(a, (a.shape[0], 1))\n",
    "        series = fit_transform(series)\n",
    "        \n",
    "        # Series for lockdown levels and population density\n",
    "#         ld_series = to_categorical(tup[2])\n",
    "#         state_pop_density = tup[3]\n",
    "#         pd_series = np.repeat(tup[3], len(series))\n",
    "#         pd_series = np.reshape(pd_series, (pd_series.shape[0], 1))\n",
    "        \n",
    "        num_episodes = len(series) - episode_length + 1\n",
    "        if num_episodes < 2: continue\n",
    "\n",
    "        state_x = []\n",
    "        state_y = []\n",
    "        for _in in range(num_episodes-look_ahead):\n",
    "            # Concatenate infected numbers and lockdown levels\n",
    "            multivar_x = series[_in:_in+episode_length]\n",
    "#             multivar_x = np.concatenate((series[_in:_in+episode_length], ld_series[\n",
    "#                 _in:_in+episode_length]), axis=1)\n",
    "            # Also concatenate population density along with these numbers\n",
    "            # multivar_x = np.concatenate((multivar_x, pd_series[_in:_in+episode_length]), axis=1)\n",
    "            state_x.append(multivar_x)\n",
    "            state_y.append(series[_in+episode_length+look_ahead-1])\n",
    "\n",
    "        # Training and testing set\n",
    "        train_length = int(train_percent*len(state_x))\n",
    "        global x_arange_train\n",
    "        global x_arange_test\n",
    "        \n",
    "        x_arange = np.arange(1, len(state_x)+1).tolist()\n",
    "        d = list(zip(state_x, state_y, x_arange))\n",
    "        random.shuffle(d)\n",
    "        state_x, state_y, x_arange = zip(*d)\n",
    "\n",
    "        # Add to State wise set\n",
    "        state_set.append((tup[0], state_x, state_y))\n",
    "\n",
    "        x_train.extend(state_x[:train_length])\n",
    "        y_train.extend(state_y[:train_length])\n",
    "        x_arange_train.extend(x_arange[:train_length])\n",
    "        \n",
    "        x_test.extend(state_x[train_length:-1])\n",
    "        y_test.extend(state_y[train_length:-1])\n",
    "        x_arange_test.extend(x_arange[train_length:-1])\n",
    "        \n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    print (x_train.shape, x_test.shape)\n",
    "    return (x_train, y_train), (x_test, y_test), state_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = ['a', 'b', 'c']\n",
    "# b = [1, 2, 3]\n",
    "# c = [4, 20, 35]\n",
    "# d = list(zip(a, b,c))\n",
    "# random.shuffle(d)\n",
    "# a, b, c = zip(*d)\n",
    "# print(a, b, c)\n",
    "\n",
    "#         state_x = np.array(state_x)\n",
    "#         state_y = np.array(state_y)\n",
    "#         np.random.shuffle(state_x)\n",
    "#         np.random.shuffle(state_y)\n",
    "#         state_x = state_x.tolist()\n",
    "#         state_y = state_y.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model over state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 14\n",
    "num_features = 1\n",
    "x_arange_train = []\n",
    "x_arange_test = []\n",
    "train_percent = 0.75\n",
    "\n",
    "if lstm_model:\n",
    "    train, test, state_set = divide_state_series(state_series, train_percent=train_percent, look_ahead=7)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    np.random.seed(7)\n",
    "    trainX = np.reshape(train[0], (train[0].shape[0], train[0].shape[1], num_features))\n",
    "    testX = np.reshape(test[0], (test[0].shape[0], test[0].shape[1], num_features))\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(2, input_shape=(look_back, 1), return_sequences=True))\n",
    "    model.add(LSTM(7, input_shape=(look_back, num_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=Huber(delta=50), optimizer='adam')\n",
    "    model.fit(trainX, train[1], epochs=40, batch_size=1, verbose=2)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = inverse_transform(trainPredict)\n",
    "    trainY = inverse_transform(train[1])\n",
    "    testPredict = inverse_transform(testPredict)\n",
    "    testY = inverse_transform(test[1])\n",
    "    \n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    dataset = dist_series[0][1]\n",
    "    a = np.array(dataset)\n",
    "    dataset = a.reshape(a.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs for each of the districts in the dist_series\n",
    "for tup in state_set:\n",
    "    state_x = np.array(tup[1])\n",
    "    state_y = np.array(tup[2])\n",
    "    train_length = int(train_percent*len(state_x))\n",
    "    \n",
    "    trainX = state_x[:train_length]\n",
    "    testX = state_x[train_length:-1]\n",
    "    trainY = state_y[:train_length]\n",
    "    testY = state_y[train_length:-1]\n",
    "    \n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], num_features))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], num_features))\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = inverse_transform(trainPredict)\n",
    "    trainY = inverse_transform(trainY)\n",
    "    testPredict = inverse_transform(testPredict)\n",
    "    testY = inverse_transform(testY)\n",
    "\n",
    "    print(trainY.shape, trainPredict.shape)\n",
    "    print(testY.shape, testPredict.shape)\n",
    "    trueY = trainY.tolist() + testY.tolist()\n",
    "    predictY = trainPredict.tolist() + testPredict.tolist()\n",
    "    x = x_arange_train + x_arange_test\n",
    "    zipped_pairs = zip(x, trueY, predictY) \n",
    "    xnew = [x for x, yt, yp in sorted(zipped_pairs)] \n",
    "    zipped_pairs = zip(x, trueY, predictY)\n",
    "    trueYnew = [yt for x, yt, yp in sorted(zipped_pairs)] \n",
    "    zipped_pairs = zip(x, trueY, predictY)\n",
    "    predictYnew = [yp for x, yt, yp in sorted(zipped_pairs)] \n",
    "\n",
    "    print(xnew, trueYnew, predictYnew)\n",
    "#     trainYnew = []\n",
    "#     trainPredictnew = []\n",
    "#     testYnew = []\n",
    "#     testPredictnew = []\n",
    "#     x1 = np.arange(1, trainY.shape[0]+1)\n",
    "#     x2 = np.arange(trainY.shape[0]+1, trainY.shape[0]+1+testY.shape[0])\n",
    "                \n",
    "    plt.plot(xnew, trueYnew)\n",
    "    plt.plot(xnew, predictYnew)\n",
    "    \n",
    "    print (tup[0] + ':')\n",
    "    # Time Series\n",
    "#     plt.plot(x1, trainYnew)\n",
    "#     plt.plot(x1, trainPredictnew)\n",
    "#     plt.plot(x2, testYnew)\n",
    "#     plt.plot(x2, testPredictnew)\n",
    "    plt.show()\n",
    "\n",
    "    # Cumulative\n",
    "#     x = np.append(x1, x2)\n",
    "#     y_true = np.append(trainY, testY)\n",
    "#     y_pred = np.append(trainPredict, testPredict)\n",
    "    plt.plot(xnew, np.cumsum(trueYnew))\n",
    "    plt.plot(xnew, np.cumsum(predictYnew))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = state_series[0][1]\n",
    "a = np.array(series)\n",
    "series = np.reshape(a, (a.shape[0], 1))\n",
    "state_series_ex = fit_transform(series)\n",
    "\n",
    "weekX = []\n",
    "for i in range(7):\n",
    "    x_temp = []\n",
    "    for j in range(14):\n",
    "        x_temp.append(state_series_ex[ 41 +i+j  ]) #len(state_series[0][1])-1-14-7\n",
    "    weekX.append(x_temp)\n",
    "\n",
    "\n",
    "# 68-1-14-7 = 46\n",
    "# 67 - 60-(14)47\n",
    "# 61 - 54-(14)41\n",
    "\n",
    "weekX = np.array(weekX)\n",
    "weekX = np.reshape(weekX, (weekX.shape[0], weekX.shape[1], num_features))\n",
    "weekPredict = model.predict(weekX)\n",
    "\n",
    "# invert predictions\n",
    "weekPredict = inverse_transform(weekPredict)\n",
    "len(weekPredict)\n",
    "plt.plot(np.arange(len(state_series_ex)+1-7, len(state_series_ex)+1), weekPredict)\n",
    "plt.plot(np.arange(1, len(state_series_ex)+1), state_series[0][1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(weekPredict))\n",
    "print(state_series)\n",
    "plt.plot(np.arange(len(state_series)+1-7, len(state_series)+1), weekPredict)\n",
    "plt.plot(np.arange(1, len(state_series)+1), state_series[0][1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_series[0][1][len(state_series[0][1])-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
