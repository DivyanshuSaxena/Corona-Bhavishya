{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import importlib as imp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User module imports\n",
    "from utils import district_daily_data as dd\n",
    "dd = imp.reload(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "linear_reg = False\n",
    "sv_reg = True\n",
    "episodes = True\n",
    "lstm_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory variables\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read state data\n",
    "df_state = pd.read_csv(data_dir + 'state-date-total-data.csv')\n",
    "arr_state = df_state.to_numpy() # still reversed\n",
    "arr_state = np.flipud(arr_state) # now taken data from day-1 to day-52; but still daily cases\n",
    "arr_state = np.cumsum(arr_state, axis=0) # now cumulative cases till day 52\n",
    "np.savetxt(data_dir + 'state-date-total-data-cumulative.csv', arr_state.astype(int), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read population density data\n",
    "df_population_density = pd.read_csv(data_dir + 'district_wise_population_density.csv')\n",
    "np_population_density = df_population_density.to_numpy() \n",
    "data_found_count = 0 # no of districts for which we have population density data\n",
    "\n",
    "def get_district_population_density(d):\n",
    "    global data_found_count\n",
    "    dist_pop_density = -2\n",
    "    for i_cn in range(len(np_population_density)):\n",
    "        if(np_population_density[i_cn][1].lower().count(d.lower().strip()) > 0):\n",
    "            dist_pop_density = max(float(np_population_density[i_cn][7]), dist_pop_density)\n",
    "    if(dist_pop_density <= 0): # print(d) # district not matched || area not found || population data missing\n",
    "        dist_pop_density = 368 # population density of INDIA\n",
    "    else:\n",
    "        data_found_count = data_found_count + 1\n",
    "    return dist_pop_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read district data\n",
    "districts = dd.get_all_districts()\n",
    "dist_series = []  # [(start_date, series), (start_date, series), ...]\n",
    "max_number = 0\n",
    "\n",
    "# Note: start_date might itself be a feature\n",
    "for d in districts:\n",
    "    if d == \"Mumbai\" or d == \"Thane\" or d == \"Delhi\" or d == \"Lucknow\":\n",
    "        d_start_date = dd.get_infection_start(d)\n",
    "        district_pop_density = get_district_population_density(d)\n",
    "        district_time_series = dd.get_district_time_series(d, d_start_date)\n",
    "        district_max = max(district_time_series)\n",
    "        \n",
    "        dist_series.append((d_start_date, district_time_series, district_pop_density))\n",
    "        if district_max > max_number:\n",
    "            max_number = district_max\n",
    "        \n",
    "print(\"data_found_count:\", data_found_count,  \" tot dists:\", len(districts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = 5\n",
    "\n",
    "# Transform using MinMaxScaler\n",
    "def fit_transform(series):\n",
    "    global max_number\n",
    "    series = np.array(list(map(lambda x: x/max_number*feature_range, series)))\n",
    "    return series\n",
    "\n",
    "# Revert the transform to get actual series\n",
    "def inverse_transform(series):\n",
    "    global max_number\n",
    "    series = np.array(list(map(lambda x: np.rint(x*max_number/feature_range), series)))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get separate train and test sets with data points from each of the districts\n",
    "def divide_series(dist_series, train_percent, look_ahead=1):\n",
    "    # Construct train and test data and fit Support Vector Regression\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    episode_length = 14\n",
    "    count = 0\n",
    "    for tup in dist_series:\n",
    "        series = tup[1]\n",
    "        a = np.array(series)\n",
    "        series = a.reshape(a.shape[0], 1)\n",
    "        series = fit_transform(series)\n",
    "\n",
    "        dist_pop_density = tup[2]\n",
    "        num_episodes = len(series) - episode_length + 1\n",
    "        if num_episodes < 2: continue\n",
    "\n",
    "        dist_x = []\n",
    "        dist_y = []\n",
    "        for _in in range(num_episodes-look_ahead):\n",
    "            dist_x.append(series[_in:_in+episode_length])\n",
    "            dist_y.append(series[_in+episode_length+look_ahead-1])\n",
    "        \n",
    "        train_length = int(train_percent*len(dist_x))\n",
    "        x_train.extend(dist_x[:train_length])\n",
    "        y_train.extend(dist_y[:train_length])\n",
    "        x_test.extend(dist_x[train_length:-1])\n",
    "        y_test.extend(dist_y[train_length:-1])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    print (x_train.shape, x_test.shape)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lstm_model:\n",
    "    train, test = divide_series(dist_series, 0.67, look_ahead=6)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    np.random.seed(7)\n",
    "    look_back = 14\n",
    "    trainX = np.reshape(train[0], (train[0].shape[0], train[0].shape[1], 1))\n",
    "    testX = np.reshape(test[0], (test[0].shape[0], test[0].shape[1], 1))\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(2, input_shape=(look_back, 1), return_sequences=True))\n",
    "    model.add(LSTM(7, input_shape=(look_back, 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=Huber(delta=10), optimizer='adam')\n",
    "    model.fit(trainX, train[1], epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = inverse_transform(trainPredict)\n",
    "    trainY = inverse_transform(train[1])\n",
    "    testPredict = inverse_transform(testPredict)\n",
    "    testY = inverse_transform(test[1])\n",
    "    \n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    dataset = dist_series[0][1]\n",
    "    a = np.array(dataset)\n",
    "    dataset = a.reshape(a.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0-13]-> 14, 43 = trainPredict, 44, 51\n",
    "# 52= 30 8 (38)\n",
    "\n",
    "print(trainY.shape, trainPredict.shape)\n",
    "print(testY.shape, testPredict.shape)\n",
    "\n",
    "x1 = np.arange(1, trainY.shape[0]+1)\n",
    "x2 = np.arange(trainY.shape[0]+1, trainY.shape[0]+1+testY.shape[0])\n",
    "\n",
    "# Time Series\n",
    "plt.plot(x1, trainY)\n",
    "plt.plot(x1, trainPredict)\n",
    "plt.plot(x2, testY)\n",
    "plt.plot(x2, testPredict)\n",
    "plt.show()\n",
    "\n",
    "# Cumulative\n",
    "x = np.append(x1, x2)\n",
    "y_true = np.append(trainY, testY)\n",
    "y_pred = np.append(trainPredict, testPredict)\n",
    "plt.plot(x, np.cumsum(y_true))\n",
    "plt.plot(x, np.cumsum(y_pred))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
