{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import importlib as imp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.losses import Huber\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User module imports\n",
    "from utils import district_daily_data as dd\n",
    "from utils import get_lockdown as gl\n",
    "dd = imp.reload(dd)\n",
    "gl = imp.reload(gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "lstm_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory variables\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read state data\n",
    "df_state = pd.read_csv(data_dir + 'state-date-total-data.csv')\n",
    "arr_state = df_state.to_numpy() # still reversed\n",
    "arr_state = np.flipud(arr_state) # now taken data from day-1 to day-52; but still daily cases\n",
    "arr_state = np.cumsum(arr_state, axis=0) # now cumulative cases till day 52\n",
    "np.savetxt(data_dir + 'state-date-total-data-cumulative.csv', arr_state.astype(int), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read population density data\n",
    "df_population_density = pd.read_csv(data_dir + 'district_wise_population_density.csv')\n",
    "np_population_density = df_population_density.to_numpy() \n",
    "data_found_count = 0 # no of districts for which we have population density data\n",
    "\n",
    "def get_district_population_density(d):\n",
    "    global data_found_count\n",
    "    dist_pop_density = -2\n",
    "    for i_cn in range(len(np_population_density)):\n",
    "        if(np_population_density[i_cn][1].lower().count(d.lower().strip()) > 0):\n",
    "            dist_pop_density = max(float(np_population_density[i_cn][7]), dist_pop_density)\n",
    "    if(dist_pop_density <= 0): # print(d) # district not matched || area not found || population data missing\n",
    "        dist_pop_density = 368 # population density of INDIA\n",
    "    else:\n",
    "        data_found_count = data_found_count + 1\n",
    "    return dist_pop_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read district data\n",
    "districts = dd.get_all_districts()\n",
    "dist_series = []  # [(start_date, series), (start_date, series), ...]\n",
    "max_number = 0\n",
    "\n",
    "# Note: start_date might itself be a feature\n",
    "for d in districts:\n",
    "    if d == \"Mumbai\" or d == \"Thane\" or d == \"Delhi\" or d == \"Lucknow\":\n",
    "#     if d == \"Mumbai\":\n",
    "        d_start_date = dd.get_infection_start(d)\n",
    "        district_pop_density = get_district_population_density(d)\n",
    "        district_time_series = dd.get_district_time_series(d, d_start_date)\n",
    "        lockdown_time_series = gl.get_lockdown_series(d, d_start_date, len(district_time_series))\n",
    "        dist_series.append((d, district_time_series, lockdown_time_series, district_pop_density))\n",
    "        \n",
    "        # Update district max to scale all numbers\n",
    "        district_max = max(district_time_series)\n",
    "        if district_max > max_number:\n",
    "            max_number = district_max\n",
    "\n",
    "print(dist_series)\n",
    "print(\"data_found_count:\", data_found_count,  \" tot dists:\", len(districts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = 5\n",
    "\n",
    "# Transform using MinMaxScaler\n",
    "def fit_transform(series):\n",
    "    global max_number\n",
    "    series = np.array(list(map(lambda x: x/max_number*feature_range, series)))\n",
    "    return series\n",
    "\n",
    "# Revert the transform to get actual series\n",
    "def inverse_transform(series):\n",
    "    global max_number\n",
    "    series = np.array(list(map(lambda x: np.rint(x*max_number/feature_range), series)))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get separate train and test sets with data points from each of the districts\n",
    "def divide_series(dist_series, train_percent, look_ahead=1):\n",
    "    # Construct train and test data and fit Support Vector Regression\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    # Construct a list of district wise input features and outputs\n",
    "    dist_set = []\n",
    "    episode_length = 14\n",
    "    count = 0\n",
    "    for tup in dist_series:\n",
    "        # Series for infected numbers\n",
    "        series = tup[1]\n",
    "        a = np.array(series)\n",
    "        series = np.reshape(a, (a.shape[0], 1))\n",
    "        series = fit_transform(series)\n",
    "        \n",
    "        # Series for lockdown levels and population density\n",
    "        ld_series = to_categorical(tup[2])\n",
    "        dist_pop_density = tup[3]\n",
    "        pd_series = np.repeat(tup[3], len(series))\n",
    "        pd_series = np.reshape(pd_series, (pd_series.shape[0], 1))\n",
    "        \n",
    "        num_episodes = len(series) - episode_length + 1\n",
    "        if num_episodes < 2: continue\n",
    "\n",
    "        dist_x = []\n",
    "        dist_y = []\n",
    "        for _in in range(num_episodes-look_ahead):\n",
    "            # Concatenate infected numbers and lockdown levels\n",
    "            multivar_x = np.concatenate((series[_in:_in+episode_length], ld_series[\n",
    "                _in:_in+episode_length]), axis=1)\n",
    "            # Also concatenate population density along with these numbers\n",
    "            # multivar_x = np.concatenate((multivar_x, pd_series[_in:_in+episode_length]), axis=1)\n",
    "            dist_x.append(multivar_x)\n",
    "            dist_y.append(series[_in+episode_length+look_ahead-1])\n",
    "        \n",
    "        # Add to Dsitrict wise set\n",
    "        dist_set.append((tup[0], dist_x, dist_y))\n",
    "        \n",
    "        # Training and testing set\n",
    "        train_length = int(train_percent*len(dist_x))\n",
    "        x_train.extend(dist_x[:train_length])\n",
    "        y_train.extend(dist_y[:train_length])\n",
    "        x_test.extend(dist_x[train_length:-1])\n",
    "        y_test.extend(dist_y[train_length:-1])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    print (x_train.shape, x_test.shape)\n",
    "    return (x_train, y_train), (x_test, y_test), dist_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 14\n",
    "num_features = 7\n",
    "train_percent = 0.75\n",
    "\n",
    "if lstm_model:\n",
    "    train, test, dist_set = divide_series(dist_series, train_percent=train_percent, look_ahead=6)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    np.random.seed(7)\n",
    "    trainX = np.reshape(train[0], (train[0].shape[0], train[0].shape[1], num_features))\n",
    "    testX = np.reshape(test[0], (test[0].shape[0], test[0].shape[1], num_features))\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(2, input_shape=(look_back, 1), return_sequences=True))\n",
    "    model.add(LSTM(7, input_shape=(look_back, num_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=Huber(delta=50), optimizer='adam')\n",
    "    model.fit(trainX, train[1], epochs=150, batch_size=1, verbose=2)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = inverse_transform(trainPredict)\n",
    "    trainY = inverse_transform(train[1])\n",
    "    testPredict = inverse_transform(testPredict)\n",
    "    testY = inverse_transform(test[1])\n",
    "    \n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    dataset = dist_series[0][1]\n",
    "    a = np.array(dataset)\n",
    "    dataset = a.reshape(a.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs for each of the districts in the dist_series\n",
    "for tup in dist_set:\n",
    "    dist_x = np.array(tup[1])\n",
    "    dist_y = np.array(tup[2])\n",
    "    train_length = int(train_percent*len(dist_x))\n",
    "    \n",
    "    trainX = dist_x[:train_length]\n",
    "    testX = dist_x[train_length:-1]\n",
    "    trainY = dist_y[:train_length]\n",
    "    testY = dist_y[train_length:-1]\n",
    "    \n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], num_features))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], num_features))\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = inverse_transform(trainPredict)\n",
    "    trainY = inverse_transform(trainY)\n",
    "    testPredict = inverse_transform(testPredict)\n",
    "    testY = inverse_transform(testY)\n",
    "\n",
    "    print(trainY.shape, trainPredict.shape)\n",
    "    print(testY.shape, testPredict.shape)\n",
    "\n",
    "    x1 = np.arange(1, trainY.shape[0]+1)\n",
    "    x2 = np.arange(trainY.shape[0]+1, trainY.shape[0]+1+testY.shape[0])\n",
    "\n",
    "    print (tup[0] + ':')\n",
    "    # Time Series\n",
    "    plt.plot(x1, trainY)\n",
    "    plt.plot(x1, trainPredict)\n",
    "    plt.plot(x2, testY)\n",
    "    plt.plot(x2, testPredict)\n",
    "    plt.show()\n",
    "\n",
    "    # Cumulative\n",
    "    x = np.append(x1, x2)\n",
    "    y_true = np.append(trainY, testY)\n",
    "    y_pred = np.append(trainPredict, testPredict)\n",
    "    plt.plot(x, np.cumsum(y_true))\n",
    "    plt.plot(x, np.cumsum(y_pred))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
