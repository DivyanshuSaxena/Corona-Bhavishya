{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import importlib as imp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User module imports\n",
    "from utils import district_daily_data as dd\n",
    "dd = imp.reload(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "linear_reg = False\n",
    "sv_reg = True\n",
    "episodes = True\n",
    "lstm_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory variables\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read state data\n",
    "df_state = pd.read_csv(data_dir + 'state-date-total-data.csv')\n",
    "arr_state = df_state.to_numpy() # still reversed\n",
    "arr_state = np.flipud(arr_state) # now taken data from day-1 to day-52; but still daily cases\n",
    "arr_state = np.cumsum(arr_state, axis=0) # now cumulative cases till day 52\n",
    "np.savetxt(data_dir + 'state-date-total-data-cumulative.csv', arr_state.astype(int), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read population density data\n",
    "df_population_density = pd.read_csv(data_dir + 'district_wise_population_density.csv')\n",
    "np_population_density = df_population_density.to_numpy() \n",
    "data_found_count = 0 # no of districts for which we have population density data\n",
    "\n",
    "def get_district_population_density(d):\n",
    "    global data_found_count\n",
    "    dist_pop_density = -2\n",
    "    for i_cn in range(len(np_population_density)):\n",
    "        if(np_population_density[i_cn][1].lower().count(d.lower().strip()) > 0):\n",
    "            dist_pop_density = max(float(np_population_density[i_cn][7]), dist_pop_density)\n",
    "    if(dist_pop_density <= 0): # print(d) # district not matched || area not found || population data missing\n",
    "        dist_pop_density = 368 # population density of INDIA\n",
    "    else:\n",
    "        data_found_count = data_found_count + 1\n",
    "    return dist_pop_density\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read district data\n",
    "districts = dd.get_all_districts()\n",
    "dist_series = []  # [(start_date, series), (start_date, series), ...]\n",
    "\n",
    "# Note: start_date might itself be a feature\n",
    "for d in districts:\n",
    "    if d == \"Mumbai\":\n",
    "        d_start_date = dd.get_infection_start(d)\n",
    "        district_pop_density = get_district_population_density(d)\n",
    "        dist_series.append((d_start_date, dd.get_district_time_series(d, d_start_date), district_pop_density))\n",
    "#     print(d_start_date)\n",
    "\n",
    "print(\"data_found_count:\", data_found_count,  \" tot dists:\", len(districts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if linear_reg:\n",
    "    # Get x and y plots - LinearRegression\n",
    "    X = np.arange(1,53)\n",
    "    X = np.reshape(X, (52,1))\n",
    "    print (arr_state.shape)\n",
    "\n",
    "    for i in range(len(arr_state[0])):\n",
    "        y = arr_state[:,i]\n",
    "        y = np.reshape(y, (52,1))\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        x_test = np.arange(1,56).reshape(55,1)\n",
    "        y_test = reg.predict(x_test)\n",
    "        plt.scatter(X, y, color='black')\n",
    "        plt.plot(x_test,y_test)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sv_reg and not episodes:\n",
    "    # Get x and y plots - SVRegression\n",
    "    X = np.arange(1,53)\n",
    "    X = np.reshape(X, (52,1))\n",
    "\n",
    "    for i in range(len(arr_state[0])):\n",
    "        y = arr_state[:,i]\n",
    "        x_test = np.arange(1,56).reshape(55,1)\n",
    "        clf = SVR(C=100.0, gamma=100)\n",
    "        clf.fit(X, y)\n",
    "        y_test = clf.predict(x_test)\n",
    "\n",
    "        plt.scatter(X, y, color='black')\n",
    "        plt.plot(x_test,y_test)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if episodes:\n",
    "    # Construct train and test data and fit Support Vector Regression\n",
    "    x = []\n",
    "    y = []\n",
    "    episode_length = 14\n",
    "    count = 0\n",
    "    for tup in dist_series:\n",
    "        series = tup[1]\n",
    "        dist_pop_density = tup[2]\n",
    "        print (len(series))\n",
    "        num_episodes = len(series) - episode_length + 1\n",
    "        if num_episodes < 2: continue\n",
    "        print (num_episodes)\n",
    "        for _in in range(num_episodes-1):\n",
    "            x.append([dist_pop_density] + series[_in:_in+episode_length])\n",
    "            y.append(series[_in+episode_length])\n",
    "    print (len(x))\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    train_length = int(0.8*len(x))\n",
    "    x_train = x[:train_length]\n",
    "    y_train = y[:train_length]\n",
    "    x_test = x[train_length:]\n",
    "    y_true = y[train_length:]\n",
    "    clf = SVR(C=100.0, gamma='scale')\n",
    "    clf.fit(x, y)\n",
    "    \n",
    "    y_test = clf.predict(x)\n",
    "    X = np.arange(len(y))\n",
    "    X = np.reshape(X, (len(y), 1))\n",
    "    \n",
    "    # Test\n",
    "    # y_test = clf.predict(x_test)\n",
    "    # X = np.arange(len(y_test))\n",
    "    # X = np.reshape(X, (len(y_test), 1))\n",
    "    plt.plot(X, y, color='black')\n",
    "    plt.plot(X, y_test, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (x_train[0])\n",
    "# type(np.array(series))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lstm_model:\n",
    "    # Construct train and test data and fit Support Vector Regression\n",
    "    x = []\n",
    "    y = []\n",
    "    episode_length = 14\n",
    "    count = 0\n",
    "    for tup in dist_series:\n",
    "        series = tup[1]\n",
    "        a = np.array(series)\n",
    "        series = a.reshape(a.shape[0], 1)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        series = scaler.fit_transform(series)\n",
    "\n",
    "        dist_pop_density = tup[2] \n",
    "        print (len(series))\n",
    "        num_episodes = len(series) - episode_length + 1\n",
    "        if num_episodes < 2: continue\n",
    "        print (num_episodes)\n",
    "        for _in in range(num_episodes-1-6):\n",
    "            x.append(series[_in:_in+episode_length])\n",
    "            y.append(series[_in+episode_length+6])\n",
    "    print (len(x))\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    train_length = int(0.67*len(x))\n",
    "    x_train = x[:train_length]\n",
    "    y_train = y[:train_length]\n",
    "    x_test = x[train_length:]\n",
    "    y_true = y[train_length:]\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    np.random.seed(7)\n",
    "    look_back = 14\n",
    "    trainX = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    testX = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(y_train)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(y_true)\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    dataset = dist_series[0][1]\n",
    "    a = np.array(dataset)\n",
    "    dataset = a.reshape(a.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0-13]-> 14, 43 = trainPredict, 44, 51\n",
    "# 52= 30 8 (38)\n",
    "\n",
    "print(trainY.shape, trainPredict.shape)\n",
    "print(testY.shape, testPredict.shape)\n",
    "\n",
    "x1 = np.arange(1, trainY.shape[0]+1)\n",
    "x2 = np.arange(trainY.shape[0]+1, trainY.shape[0]+1+testY.shape[0])\n",
    "\n",
    "# plt.plot(x1, trainY)\n",
    "# plt.plot(x1, trainPredict)\n",
    "plt.plot(x2, testY)\n",
    "plt.plot(x2, testPredict)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainY[:,0])\n",
    "# print(trainPredict[:, 0])\n",
    "# dataset.shape\n",
    "# trainX.shape\n",
    "# testX.shape\n",
    "# plt.plot(dataset)\n",
    "# plt.plot(trainPredictPlot)\n",
    "# plt.plot(testPredictPlot)\n",
    "# plt.show()\n",
    "# trainPredictPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
